# Дронопорт
Оптимизация зарядки дрона Yandex на электростанции

ЦЕЛЬ КЕЙСА
Разработать и сравнить два RL-агента (рациональный Q-Learning и поведенческий) для задачи быстрой зарядки одного дрона на электростанции с учетом ветра, энергопотребления и статичных препятствий

ГЕОМЕТРИЯ И ДИНАМИКА ЗАДАЧИ

Пространство движения: x:, y:

Объекты:

Электрозаправка: центр (18.0, 6.0), радиус 0.7м, мощность зарядки 0.02/шаг

Начальная позиция дрона: нормальное распределение N(2.5,5.0; σ=1.5), ограничение [0.5,6.0]

Ветер: вектор [-0.08,+0.08] м/шаг, обновление каждые 20 шагов

Статичные препятствия (прямоугольники):

(12.5,3.0)-(13.5,4.5)

(14.0,7.5)-(15.0,9.0)

(16.5,2.0)-(17.5,5.0)

Состояние (4D): (dx_к_заправке, dy_к_заправке, заряд∈, скорость∈{0.08,0.22,0.38,0.55})
Действия: 16 дискретных (4 направления × 3 скорости)

Динамика:
dx_new = dx_old + vcos(θ) + wind_x
dy_new = dy_old + vsin(θ) + wind_y
заряд_new = заряд_old - 0.015v² - 0.002wind²

Условия завершения:
УСПЕХ: расстояние<0.7м И заряд>0.92 (макс. 120 шагов)
ПРОВАЛ: заряд<0.05 ИЛИ 180 шагов

Награда:
+80 - успех
-150 - батарея_кончилась
-0.8 - шаг
-0.025v² - энергозатраты
+0.1мощность_зарядки - на станции




Пункт 1.1. Среда  

Случайный старт из нормального распределения

Ветер с периодическим обновлением

Коллизии с 3 препятствиями

Нелинейная динамика батареи

Дискретизация состояний (12×12×10×4=5760)



Пункт 1.2. Рациональный Q-Learning

Табличный Q-learning, ε-greedy (1.0→0.01, decay=0.997)

15 000 эпизодов обучения



Пункт 1.3. Поведенческая модель

 (α=0.88, β=0.88, λ=2.35)

15 000 эпизодов обучения



Пункт 1.4. Оценка и визуализация 

300 тестов на агента (ε=0.005)

Метрики: успехи%, время, энергопотребление, коллизии%

Кривая обучения (окно 150)

CDF времени успеха

Heatmap финального заряда

4 траектории Rational

4 траектории Prospect

Таблица + t-test (p<0.05)

